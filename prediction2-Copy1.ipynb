{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/workstation/PycharmProjects/data'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "base_dir = os.path.abspath('../data')      \n",
    "data_file = base_dir + r'/RunDataTable.tsv'\n",
    "relationship_def_file = base_dir + r'/RelationshipDefTable.tsv'\n",
    "relationship_data_file = base_dir + r'/RelationshipDataTable.tsv'\n",
    "summary_statistics_data_file = base_dir + r'/SummaryStatisticsDataTable.tsv'\n",
    "variable_def_table = base_dir + r'/VariableDefTable.tsv'\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f9868d5a549a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VariableName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/workstation/PycharmProjects/data/RunDataTable.tsv' does not exist: b'/home/workstation/PycharmProjects/data/RunDataTable.tsv'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/workstation/PycharmProjects/data/RunDataTable.tsv' does not exist: b'/home/workstation/PycharmProjects/data/RunDataTable.tsv'",
     "output_type": "error"
    }
   ],
   "source": [
    "data = pd.read_table(data_file,parse_dates=True)\n",
    "data['VariableName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "temp=data[data['VariableName']=='SiteLocation']\n",
    "# meeting_data = temp[temp['EntityIdx']=='Site-2629']\n",
    "# data[data['VariableName']=='OngoingMeetingId']\n",
    "temp.head(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "summary_statistics_data = pd.read_table(summary_statistics_data_file,parse_dates=True, index_col=0)\n",
    "summary_statistics_data['VariableName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "summary_statistics_data= summary_statistics_data[summary_statistics_data['VariableName']=='Population']\n",
    "summary_statistics_data[summary_statistics_data['EntityIdx']=='Neighborhood-2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_meeting_data(data, site_id):\n",
    "    meeting_data = data[data['VariableName']=='OngoingMeetingId']\n",
    "    meeting_data = meeting_data[meeting_data['EntityIdx']==site_id]\n",
    "    meeting_data = meeting_data.groupby('Value')['Timestep'].min().reset_index()\n",
    "#     print(meeting_data)\n",
    "#     meeting_data1 = meeting_data.groupby('Value')['Timestep'].max().reset_index()\n",
    "#     print(meeting_data)\n",
    "    meeting_data = meeting_data.groupby('Timestep')[['Value']].count()\n",
    "#     print(meeting_data)\n",
    "    meeting_data.index = pd.to_datetime(meeting_data.index)\n",
    "    meeting_data =  meeting_data.resample('D').sum()\n",
    "    return meeting_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_2629_meeting_data_1 = get_meeting_data(data, 'Site-2629')\n",
    "site_5298_meeting_data_1 = get_meeting_data(data, 'Site-5298')\n",
    "site_5299_meeting_data_1 = get_meeting_data(data, 'Site-5299')\n",
    "site_10648_meeting_data_1 = get_meeting_data(data, 'Site-10648')\n",
    "site_7976_meeting_data_1 = get_meeting_data(data, 'Site-7976')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_2629_meeting_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_5298_meeting_data_1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_5299_meeting_data_1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_10648_meeting_data_1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_7976_meeting_data_1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "initial_data_file = base_dir + r'/Urban World Predict Challenge Data/RunDataTable.tsv'\n",
    "rr_0260_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0260-RunDataTable.tsv'\n",
    "rr_0310_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0310-RunDataTable.tsv'\n",
    "rr_0320_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0320-RunDataTable.tsv'\n",
    "rr_0330_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0330-RunDataTable.tsv'\n",
    "rr_0340_qualitative_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0340-QualitativeDataTable.tsv'\n",
    "rr_0340_relationship_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0340-RelationshipDataTable.tsv'\n",
    "rr_0340_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0340-RunDataTable.tsv'\n",
    "rr_0370_qualitative_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0370-QualitativeDataTable.tsv'\n",
    "rr_0370_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0370-RunDataTable.tsv'\n",
    "rr_0380_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0380-RunDataTable.tsv'\n",
    "rr_0390_data_file = base_dir + r'/Urban World Predict Challenge Data/RR-0390-RunDataTable.tsv'\n",
    "idp_sup_data_file = base_dir + r'/Urban World Predict Challenge Data/IDP_SUP_RunDataTable.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "initial_data = pd.read_csv(initial_data_file,delimiter='\\t',encoding='utf-8', index_col=False)\n",
    "initial_data['VariableName'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_2629_meeting_data_2 = get_meeting_data(initial_data, 'Site-2629')\n",
    "site_5298_meeting_data_2 = get_meeting_data(initial_data, 'Site-5298')\n",
    "site_5299_meeting_data_2 = get_meeting_data(initial_data, 'Site-5299')\n",
    "site_10648_meeting_data_2 = get_meeting_data(initial_data, 'Site-10648')\n",
    "site_7976_meeting_data_2 = get_meeting_data(initial_data, 'Site-7976')\n",
    "\n",
    "site_2629_meeting_data =  pd.concat([site_2629_meeting_data_1, site_2629_meeting_data_2])\n",
    "site_5298_meeting_data =  pd.concat([site_5298_meeting_data_1, site_5298_meeting_data_2])\n",
    "site_5299_meeting_data =  pd.concat([site_5299_meeting_data_1, site_5299_meeting_data_2])\n",
    "site_10648_meeting_data =  pd.concat([site_10648_meeting_data_1, site_10648_meeting_data_2])\n",
    "site_7976_meeting_data =  pd.concat([site_7976_meeting_data_1, site_7976_meeting_data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_2629_meeting_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df=site_2629_meeting_data.reset_index()\n",
    "df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "site_5298_meeting_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "site_5299_meeting_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "site_10648_meeting_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "site_7976_meeting_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size,  single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i)\n",
    "    # Reshape data from (history_size,) to (history_size, 1)\n",
    "#     print(dataset.shape)        \n",
    "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "#     print(dataset.shape)\n",
    "#     labels.append(dataset[i+target_size])\n",
    "    if single_step:\n",
    "      labels.append(dataset[i+target_size])\n",
    "    else:\n",
    "      labels.append(dataset[i:i+target_size])\n",
    "    \n",
    "  return np.array(data), np.array(labels)\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i, step)\n",
    "    data.append(dataset[indices])\n",
    "\n",
    "    if single_step:\n",
    "      labels.append(target[i+target_size])\n",
    "    else:\n",
    "      labels.append(target[i:i+target_size])\n",
    "\n",
    "  return np.array(data), np.array(labels)\n",
    "\n",
    "def normalize(data, train_split):\n",
    "    mean = data[:TRAIN_SPLIT].mean()\n",
    "    std = data[:TRAIN_SPLIT].std()\n",
    "    data = (data-mean)/std\n",
    "    return data\n",
    "\n",
    "def create_time_steps(length):\n",
    "  time_steps = []\n",
    "  for i in range(-length, 0, 1):\n",
    "    time_steps.append(i)\n",
    "  return time_steps\n",
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "  labels = ['History', 'True Future', 'Model Prediction']\n",
    "  marker = ['.-', 'rx', 'go']\n",
    "  time_steps = create_time_steps(plot_data[0].shape[0])\n",
    "  if delta:\n",
    "    future = delta\n",
    "  else:\n",
    "    future = 0\n",
    "\n",
    "  plt.title(title)\n",
    "  for i, x in enumerate(plot_data):\n",
    "    if i:\n",
    "      plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
    "               label=labels[i])\n",
    "    else:\n",
    "      plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "  plt.legend()\n",
    "  plt.xlim([time_steps[0], (future+5)*2])\n",
    "  plt.xlabel('Time-Step')\n",
    "  return plt\n",
    "\n",
    "def multi_step_plot(history, true_future, prediction):\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  num_in = create_time_steps(len(history))\n",
    "  num_out = len(true_future)\n",
    "  STEP = 1\n",
    "\n",
    "  plt.plot(num_in, np.array(history), label='History')\n",
    "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "  if prediction.any():\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "             label='Predicted Future')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "def split_data(data, train_split, past_history, future_target, buffer_size, batch_size):\n",
    "    x_train, y_train = univariate_data(data, 0, train_split,\n",
    "                                           past_history,\n",
    "                                           future_target)\n",
    "    print('x_train shape=',x_train.shape)\n",
    "    print('y_train shape=',y_train.shape)\n",
    "    x_val, y_val = univariate_data(data, train_split, None,\n",
    "                                       past_history,\n",
    "                                       future_target)\n",
    "    \n",
    "    y_pred = x_val[:, -future_target:]\n",
    "    print('base line error:',np.mean(tf.keras.losses.mean_squared_error(y_val, y_pred)))\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_data = val_data.batch(batch_size).repeat()\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def windowed_data(dataframe, window_size, target_size, batch_size,shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataframe)\n",
    "    dataset = dataset.window(window_size+target_size, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+target_size))\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.map(lambda window: (window[:-target_size],window[-target_size:]))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def stateful_windowed_data(dataframe, window_length, target_length,):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataframe)\n",
    "    dataset = dataset.window(window_length+target_length, shift=window_length, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length+target_length))\n",
    "    dataset = dataset.batch(1)\n",
    "    dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "#     dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 100\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 100\n",
    "window_size = 7\n",
    "target_size = 7\n",
    "past_history = 8\n",
    "future_target = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# site_2629_meeting_values = normalize(site_2629_meeting_data.values, TRAIN_SPLIT)\n",
    "site_2629_meeting_values = site_2629_meeting_data.values\n",
    "site_2629_meeting_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "site_2629_train_dataset = windowed_data(site_2629_meeting_values[:TRAIN_SPLIT], window_size, target_size, BATCH_SIZE)\n",
    "site_2629_validation_dataset = windowed_data(site_2629_meeting_values[TRAIN_SPLIT:], window_size, target_size,BATCH_SIZE)\n",
    "\n",
    "for x, y in site_2629_train_dataset.take(4):\n",
    "    print('x=',x.shape, 'y=',y.shape)\n",
    "    print('x=',x.numpy(), 'y=',y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "site_2629_train_data, site_2629_val_data = split_data(site_2629_meeting_values, TRAIN_SPLIT, past_history, \n",
    "                                 future_target, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# for x, y in site_2629_train_data.take(1):\n",
    "#     print('x=',x.shape, 'y=',y.shape)\n",
    "#     print('x=',x.numpy(), 'y=',y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "multi_step_model = tf.keras.models.Sequential()\n",
    "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=(window_size,1)))\n",
    "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
    "multi_step_model.add(tf.keras.layers.Dense(target_size))\n",
    "\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 40\n",
    "\n",
    "multi_step_history = multi_step_model.fit(site_2629_train_dataset, epochs=EPOCHS,\n",
    "                      \n",
    "                      validation_data=site_2629_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs = range(len(loss))\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "  plt.title(title)\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "STEP = 1\n",
    "def multi_step_plot(history, true_future, prediction):\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  num_in = create_time_steps(len(history))\n",
    "  num_out = len(true_future)\n",
    "\n",
    "  plt.plot(num_in, np.array(history[:]), label='History')\n",
    "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "  if prediction.any():\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "             label='Predicted Future')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for x, y in site_2629_val_data.take(3):\n",
    "#     print('x=',x,'y=',y, 'predict=', multi_step_model.predict(x))\n",
    "    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    tf.keras.layers.SimpleRNN(20),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 25\n",
    "\n",
    "history = model.fit(site_2629_train_dataset, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=site_2629_validation_dataset, validation_steps=200)\n",
    "\n",
    "plot_train_history(history, 'Multi-Step Training and validation loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.GRU(32,\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     input_shape=(None, 1)))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\n",
    "history = model.fit(site_2629_train_dataset,\n",
    "                              steps_per_epoch=200,\n",
    "                                  epochs=40,\n",
    "                              validation_data=site_2629_validation_dataset,\n",
    "                              validation_steps=200)\n",
    "plot_train_history(history, 'Multi-Step Training and validation loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "stateful_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(32, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[1, None, 1]),\n",
    "    tf.keras.layers.GRU(32, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(target_size))\n",
    "])\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "        \n",
    "stateful_model.compile(loss='mae', optimizer='adam')\n",
    "stateful_history = stateful_model.fit(site_2629_train_dataset, epochs=500, validation_data=site_2629_validation_dataset,\n",
    "          callbacks=[ResetStatesCallback()])\n",
    "\n",
    "plot_train_history(stateful_history, 'Multi-Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "stateful_model.predict(site_2629_meeting_values[np.newaxis, 10:20].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.GRU(64,\n",
    "#                      dropout=0.1,\n",
    "#                      recurrent_dropout=0.1,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, 1)))\n",
    "model.add(tf.keras.layers.GRU(32, activation='relu',\n",
    "#                      dropout=0.1\n",
    "#                      recurrent_dropout=0.1))\n",
    "                             ))\n",
    "model.add(tf.keras.layers.Dense(target_size, activation='relu'))\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0)\n",
    "# model.compile(loss=tf.keras.losses.Huber(),\n",
    "#               optimizer=optimizer,\n",
    "#               metrics=[\"mae\"])\n",
    "# history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\n",
    "history = model.fit(site_2629_train_dataset,\n",
    "#                               steps_per_epoch=200,\n",
    "                              epochs=600,\n",
    "                              validation_data=site_2629_validation_dataset,\n",
    "#                     callbacks=[lr_schedule]\n",
    "#                               validation_steps=200\n",
    "                   )\n",
    "plot_train_history(history, 'Multi-Step Training and validation loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1e-4, 0, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model.predict(site_2629_meeting_values[np.newaxis,0:8].astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### type(site_2629_meeting_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "def p_day_forecast(model, data, site, window_size, target_size):\n",
    "    last_date=data.index[-1]\n",
    "    after_p_day = datetime(2022, 7, 15)\n",
    "    delta = after_p_day-last_date\n",
    "    x = data.values[-window_size:]\n",
    "    for time in range (0, 30 + delta.days,target_size):\n",
    "        pred = model.predict(X[np.newaxis, time:time+window_size].astype(np.float32)) \n",
    "        pred = pred.reshape((target_size,1))\n",
    "        pred=np.rint(pred) \n",
    "        x = np.concatenate([x, pred], axis=0)\n",
    "    \n",
    "    index=pd.date_range(data.index[-window_size], periods=X.shape[0])\n",
    "    x=pd.DataFrame(x,index=index, columns=['Value'])\n",
    "    x.index.rename('Timestep',inplace=True)\n",
    "    x = x[after_p_day:after_p_day+timedelta(30)]\n",
    "    x['Scenario']='Scenario 1'\n",
    "    x['Site'] = site\n",
    "    x = x.reset_index()\n",
    "    x=x[['Scenario','Site','Timestep','Value']]\n",
    "    return x\n",
    "\n",
    "p_day_forecast(model, site_2629_meeting_data,'Site-2629', window_size, target_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "forecast = []\n",
    "for time in range(0, len(site_2629_meeting_values) - window_size,target_size):\n",
    "    print('time:', time)\n",
    "    pred = model.predict(site_2629_meeting_values[time:time + window_size][np.newaxis].astype(np.float32))\n",
    "    print(pred)\n",
    "    pred = pred.reshape(target_size)\n",
    "    print(pred)\n",
    "    forecast.append(pred)\n",
    "print(np.array(forecast))\n",
    "forecast = np.array(forecast).reshape(-1)\n",
    "print(forecast.shape)\n",
    "\n",
    "# forecast = forecast[TRAIN_SPLIT-window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# forecast = pd.DataFrame(forecast)\n",
    "# print(forecast)\n",
    "site_2629_meeting_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "res = pd.concat([site_2629_meeting_data[window_size:].reset_index()['Value'], pd.DataFrame(forecast)], axis =1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "res.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = site_2629_meeting_values[-window_size:]\n",
    "\n",
    "for time in range (0,30,target_size):\n",
    "    pred = model.predict(X[np.newaxis, time:time+window_size].astype(np.float32))\n",
    "    print(X.shape)\n",
    "    print(pred)\n",
    "    pred = pred.reshape((target_size,1))\n",
    "    print(pred)\n",
    "    X = np.concatenate([X, pred], axis=0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    " pd.DataFrame(forecast).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "r=np.concatenate([forecast[:,np.newaxis], X[window_size:]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "res = pd.concat([site_2629_meeting_data[window_size:].reset_index()['Value'], pd.DataFrame(r)], axis =1)\n",
    "res.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}